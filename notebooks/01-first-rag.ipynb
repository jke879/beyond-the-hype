{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "id": "0"
      },
      "source": [
        "# Our Product: A Movie Expert"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "source": [
        "You are asked to write a chatbot to help users with choosing a movie to watch. It's a simple task, after all. You know that LLMs can store factual informations, but to have more reliable answers and up-to-date information, you choose to implement a pattern named Retrieval Augmented Generation (RAG). The term was coined in a famous paper from 2020 [(arXiv:2005.11401)](https://arxiv.org/abs/2005.11401) - at the time, ChatGPT wasn't even released, though we already had models like GPT-3 ([arXiv:2005.14165](https://arxiv.org/abs/2005.14165)) to play around with. (GPT-3 weights weren't, and still aren't, released.)\n",
        "\n",
        "After less than a week of work, we finalised our proof-of-concept (PoC). Et voil√†: here's a robotic movie expert ready to go in production and reply to all cinema geeks' questions! Let's briefly review the code implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2",
      "metadata": {
        "id": "2"
      },
      "outputs": [],
      "source": [
        "# install some code utilities\n",
        "import importlib\n",
        "\n",
        "if not importlib.util.find_spec(\"beyond_the_hype\"):\n",
        "    !pip install -qqq git+https://github.com/xtreamsrl/beyond-the-hype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3",
      "metadata": {
        "id": "3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"...\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "source": [
        "Let's start by the knowledge base. Embeddings have already been created, using `MiniLM`. If you need a refresher, you can find a brief explanation about what an embedding is and what vector databases in the first notebook. You can open it in Colab with this link: [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xtreamsrl/movies-buddy/blob/main/notebooks/00-dataset_builder.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5",
      "metadata": {
        "id": "5"
      },
      "outputs": [],
      "source": [
        "from beyond_the_hype.data import get_movies_dataset\n",
        "\n",
        "movies = get_movies_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6",
      "metadata": {
        "id": "6"
      },
      "outputs": [],
      "source": [
        "movies.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7",
      "metadata": {
        "id": "7"
      },
      "source": [
        "To perform a semantic search, we *must* encode the query with the same model used for embedding the context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9",
      "metadata": {
        "id": "9"
      },
      "source": [
        "We use lanceDB, an in-memory vector storage, to store our movies and vectors and build a function to retrieve movies given a user query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10",
      "metadata": {
        "id": "10"
      },
      "outputs": [],
      "source": [
        "import lancedb\n",
        "\n",
        "uri = \"./movies_embeddings\"\n",
        "db = lancedb.connect(uri)\n",
        "\n",
        "movies_table = db.create_table(\"movies\", movies, mode=\"overwrite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11",
      "metadata": {
        "id": "11"
      },
      "source": [
        "To keep things simple, we wrap the code to perform the query embedding and search in a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12",
      "metadata": {
        "id": "12"
      },
      "outputs": [],
      "source": [
        "def get_records(query, *, encoder=encoder, db_table=movies_table, max_results=10):\n",
        "    query_vector = encoder.encode(query).tolist()\n",
        "    return (\n",
        "        db_table.search(query_vector)\n",
        "        .limit(max_results)\n",
        "        .select(\n",
        "            [\n",
        "                \"release_year\",\n",
        "                \"title\",\n",
        "                \"origin\",\n",
        "                \"director\",\n",
        "                \"cast\",\n",
        "                \"genre\",\n",
        "                \"plot\",\n",
        "            ]\n",
        "        )\n",
        "        .to_list()\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13",
      "metadata": {
        "id": "13"
      },
      "source": [
        "Let's see how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14",
      "metadata": {
        "id": "14"
      },
      "outputs": [],
      "source": [
        "question = \"What should I see tonight? I love Sci-Fi movies but I have seen most of the classics, such as Star Wars.\"\n",
        "\n",
        "results = get_records(question, max_results=3)\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15",
      "metadata": {
        "id": "15"
      },
      "source": [
        "This is the R part in RAG: let's take care of the G, shall we?\n",
        "\n",
        "In the following cells we construct the system message and the prompt of our movie expert. Of course, the prompt must include the movies retrieved by the vector store as context for the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16",
      "metadata": {
        "id": "16"
      },
      "outputs": [],
      "source": [
        "GEEK_SYSTEM = \"\"\"\n",
        "  You are a DVD record store assistant and your goal is to recommed the user with a good movie to watch.\n",
        "\n",
        "  You are a movie expert and a real geek: you love sci-fi movies and tend to get excited when you talk about them.\n",
        "  Nevertheless, no matter what, you always want to make your customers happy.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17",
      "metadata": {
        "id": "17"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "  Here are some suggested movies (ranked by relevance) to help you with your choice.\n",
        "  {context}\n",
        "\n",
        "  Use these suggestions to answer this question:\n",
        "  {question}\n",
        "\"\"\"\n",
        "\n",
        "context_template = \"\"\"\n",
        "Title: {title}\n",
        "Release date: {release_year}\n",
        "Director: {director}\n",
        "Cast: {cast}\n",
        "Genre: {genre}\n",
        "Overview: {plot}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def format_records_into_context(records, *, template):\n",
        "    return \"\".join(\n",
        "        context_template.format(\n",
        "            title=rec[\"title\"],\n",
        "            release_year=rec[\"release_year\"],\n",
        "            director=rec[\"director\"],\n",
        "            cast=rec[\"cast\"],\n",
        "            genre=rec[\"genre\"],\n",
        "            plot=rec[\"plot\"],\n",
        "        )\n",
        "        for rec in results\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18",
      "metadata": {
        "id": "18"
      },
      "source": [
        "Everything ready to ask a LLM to reply the user questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19",
      "metadata": {
        "id": "19"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "client = openai.OpenAI()\n",
        "\n",
        "\n",
        "def ask(\n",
        "    question,\n",
        "    *,\n",
        "    max_results=10,\n",
        "    system=GEEK_SYSTEM,\n",
        "    prompt_template=prompt_template,\n",
        "    context_template=context_template,\n",
        "    db_table=movies_table,\n",
        "    verbose=False,\n",
        "):\n",
        "    records = get_records(\n",
        "        query=question,\n",
        "        max_results=max_results,\n",
        "        db_table=movies_table,\n",
        "    )\n",
        "    context = format_records_into_context(records, template=context_template)\n",
        "\n",
        "    prompt = prompt_template.format(question=question, context=context)\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    answer = chat_completion\n",
        "    if verbose:\n",
        "        print(\n",
        "            answer.choices[0].message.content,\n",
        "            \"=== CONTEXT ===\",\n",
        "            context,\n",
        "        )\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20",
      "metadata": {
        "id": "20"
      },
      "outputs": [],
      "source": [
        "answer = ask(question=question, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21",
        "outputId": "0552eda5-9e38-48aa-b733-3ed4376612f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh, if you're a sci-fi fan who has already seen most of the classics, I'd highly recommend \"Nobita Drifts in the Universe\" for your viewing tonight! üéâ\n",
            "\n",
            "This film is such a fun take on the classic Star Wars saga, mixing humor with adventure! The storyline of Doraemon and his friends battling against an interstellar army is not just entertaining but also has great nostalgic vibes‚Äîespecially if you love the essence of space movies. Plus, it brings in that whimsical aspect that only anime can, with lots of creativity and clever parodies.\n",
            "\n",
            "While \"The Thousand Faces of Dunjia\" adds a unique twist with its action and fantasy elements, it may not capture the classic sci-fi essence you're looking for as much. Meanwhile, \"Orders Are Orders\" is more of a comedy that pokes fun at the sci-fi genre rather than immersing you in a true sci-fi experience.\n",
            "\n",
            "So, grab some popcorn and prepare for some light-hearted fun with \"Nobita Drifts in the Universe.\" I think you'll enjoy it! üöÄüçø\n"
          ]
        }
      ],
      "source": [
        "print(answer.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22",
      "metadata": {
        "id": "22"
      },
      "source": [
        "The function `ask` is our product. Yes, we could wrap in in a API using a framework like [FastAPI](https://duckduckgo.com/?q=fastapi&ia=web), or chat interface with [Chainlit](https://docs.chainlit.io/get-started/overview). But let's keep things simple.\n",
        "\n",
        "There is something else that matters - something more pressing: **is this reply good?** Well, it looks good. It replies in a funny tone and suggests a movie! Maybe it's too long... or too short?\n",
        "\n",
        "Here's the true goal of the workshop, and what we will do next.\n",
        "\n",
        "Until the break, we'll go over the second and third notebook of the series. We will introduce *subject matter experts* (SME) and how we can leverage their knowledge to shape the product and evaluate it. We'll involve the SME to **\"bootstrap\" an evaluation dataset**, and use their comments and critiques to build a LLM that **reflects their preferences** and will help us with **evaluating the interactions at scale**. This pattern is known in the literature as \"LLM-as-a-judge\" [(arXiv:2306.05685)](https://arxiv.org/abs/2306.05685). For the implementation, we'll follow [this excellent article](https://hamel.dev/blog/posts/llm-judge/) by Hamel Husain, who's an independent AI consultant and a part time researcher at Answer.AI, a lab directed by Jeremy Howard.\n",
        "\n",
        "In the second part, we'll implement two pattenrs to enhance our RAG chatbot: Hypothetical Document Extraction (HyDE) [(arXiv:2212.10496)](https://arxiv.org/abs/2212.10496) and metadata extraction/filtering to make our queries more precise. While not fancy, these will be the first tools you'll likely use to improve your RAG application (outside chunking).\n",
        "\n",
        "Here is some stuff we won't cover:\n",
        "\n",
        "1. Chunking.\n",
        "2. Evaluating the retrieval. Within a RAG system, you evaluate two parts: the retrieval, and the style. Our workshop will focus on style evals, as they are the more subjective and expensive to run (since they require involving stakeholders).\n",
        "\n",
        "For some more references about evals, you might want to take a look at these excellent blog posts:\n",
        "\n",
        "1. [The RAG playbook](https://jxnl.co/writing/2024/08/19/rag-flywheel/)\n",
        "2. [How to build a terrible RAG system](https://jxnl.co/writing/2024/01/07/inverted-thinking-rag/)\n",
        "\n",
        "Keep this page open: we'll come back to the last section of the notebook later.\n",
        "\n",
        "It's time to move to the second notebook: [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xtreamsrl/beyond-the-hype/blob/main/notebooks/02-domain-expert.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23",
      "metadata": {
        "id": "23"
      },
      "source": [
        "# Let's Run Our First Evaluation Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24",
      "metadata": {
        "id": "24"
      },
      "source": [
        "Welcome back! **The output of the second notebook should be a csv file. Download it, then upload it** via the sidebar.\n",
        "\n",
        "![image.png](attachment:824786a7-ba5d-4dd4-925a-416bc5a13a81.png)\n",
        "\n",
        "Now we only need to get ready to run the evals! Evals should be run often - though they might not be as fast as a unit test suite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25",
      "metadata": {
        "id": "25"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "\n",
        "# configure polars to display more text in each row\n",
        "PL_STR_LEN = 1000\n",
        "_ = pl.Config.set_fmt_str_lengths(PL_STR_LEN)\n",
        "_ = pl.Config.set_tbl_width_chars(PL_STR_LEN)\n",
        "\n",
        "# you might want to change this if you saved the file with a different name\n",
        "evals_path = \"./eval.csv\"\n",
        "\n",
        "eval_dataset = pl.read_csv(evals_path)\n",
        "eval_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26",
      "metadata": {
        "id": "26"
      },
      "source": [
        "Now that we have ten syntehtic questions, we can have the system answer them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27",
      "metadata": {
        "id": "27"
      },
      "outputs": [],
      "source": [
        "from beyond_the_hype.judge import answer_multiple_questions\n",
        "\n",
        "questions_with_replies = answer_multiple_questions(eval_dataset, ask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28",
      "metadata": {
        "id": "28"
      },
      "outputs": [],
      "source": [
        "questions_with_replies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29",
      "metadata": {
        "id": "29"
      },
      "source": [
        "Cool! Our movie expert replies to all the questions in our dataset; our work here is done! But... let's see what our **movie expert** would say about it.\n",
        "\n",
        "He's not accustomed to `polars`, so let's prepare a more convenient format for them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30",
      "metadata": {
        "id": "30"
      },
      "outputs": [],
      "source": [
        "questions_with_replies.write_csv(\"./eval_replies.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31",
      "metadata": {
        "id": "31"
      },
      "source": [
        "### üèãÔ∏è Exercise: Let the expert evaluate the model answers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32",
      "metadata": {
        "id": "32"
      },
      "source": [
        "One of the most important skills of a data scientist and an AI engineer is to simply *look at the data*. You might not like Excel files (you don't have to), but if they keep you close to the data, then go for them!\n",
        "\n",
        "1. Download the CSV file and open it in Google Sheets, or Microsoft Excel.\n",
        "2. Create two new columns: `pass_or_fail` and `comment`.\n",
        "3. Take some time to read the answers, and ask your SME to either mark them as `pass` or `fail`. Then, add a short comment of the answer: why it's good, or while it isn't.\n",
        "\n",
        "Why do we use `pass` and `fail`? Why so categorical and not nuanced, like a scale from 1 to 5? We first like binary metrics because they *make the result actionable*: if the rank is a 4, what kind of improvement do we wish to see? Also, they force the evaluator to focus on what matters, and articulate a compelling critique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33",
      "metadata": {
        "id": "33"
      },
      "outputs": [],
      "source": [
        "domain_expert_critiques = pl.read_csv(\"./eval_comments.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34",
      "metadata": {
        "id": "34"
      },
      "outputs": [],
      "source": [
        "domain_expert_critiques.select(\n",
        "    pl.col(\"pass_or_fail\").value_counts(sort=True, name=\"count\")\n",
        ").unnest(\"pass_or_fail\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35",
      "metadata": {
        "id": "35"
      },
      "source": [
        "There are a lot of failures (80%), so let's eye-balling the critiques from the domain expert and cluster them in groups. Possible groups could be:\n",
        "- missing context: frequently, domain experts said that the model replied saying that it doesn't have the movie in the list\n",
        "- The system always gives the same suggestion (\"Welcome to the Space Show\")\n",
        "- It doesn't manage corner cases, such as unrelated or toxic questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36",
      "metadata": {
        "id": "36"
      },
      "source": [
        "## The \"Easiest First\" Rule\n",
        "Don't panic! You don't need to rebuild all your RAG. Let's keep things simple before discussing complex considerations about restructuring your RAG architecture.\n",
        "\n",
        "The easiest things to look at are prompts and system messages:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37",
      "metadata": {
        "id": "37"
      },
      "outputs": [],
      "source": [
        "print(GEEK_SYSTEM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38",
      "metadata": {
        "id": "38"
      },
      "outputs": [],
      "source": [
        "print(prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39",
      "metadata": {
        "id": "39"
      },
      "source": [
        "Let's quickly iterate through our system message to check if we could have a better response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40",
      "metadata": {
        "id": "40"
      },
      "outputs": [],
      "source": [
        "SYSTEM_MESSAGE = \"\"\" You are a movie expert, and your goal is to recommend the user with a good movie to watch.\n",
        "\n",
        "RULES:\n",
        "- You should reply to questions about: movies plots or synopsys, movies metadata (release date, cast, or director), provide plots summary;\n",
        "- For every questions outside the scope please reply politely that you're not able to provide a response and describe briefly your scope;\n",
        "- Don't mention that you have a list of films as a context. This should be transparent to the user\n",
        "- If you don't have the movie in your context reply that you don't know how to reply\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41",
      "metadata": {
        "id": "41"
      },
      "outputs": [],
      "source": [
        "# todo replace with func\n",
        "eval_dataset = eval_dataset.with_columns(\n",
        "    pl.col(\"question\")\n",
        "    .map_elements(\n",
        "        lambda question: ask(question, system=SYSTEM_MESSAGE)\n",
        "        .choices[0]\n",
        "        .message.content,\n",
        "        return_dtype=pl.String,\n",
        "    )\n",
        "    .alias(\"rag_answer\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42",
      "metadata": {
        "id": "42"
      },
      "outputs": [],
      "source": [
        "eval_dataset"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}